{"cells":[{"cell_type":"markdown","metadata":{"id":"UNW6_2liiKTu"},"source":["# Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31075,"status":"ok","timestamp":1693661922513,"user":{"displayName":"Hanschyo","userId":"06836591315332070494"},"user_tz":-120},"id":"ywMmRNOjhpxW","outputId":"0a7602ee-06ef-4f71-b212-eab7fcf9a660"},"outputs":[],"source":["#!pip install langdetect\n","#!pip install py-readability-metrics\n","#!pip install pyLDAvis\n","#!pip install pyngrok\n","#!pip install visdom\n","import pandas as pd\n","import nltk\n","import re\n","from bs4 import BeautifulSoup\n","from langdetect import detect\n","from readability import Readability\n","import gensim\n","import gensim.corpora as corpora\n","from gensim.models import Phrases\n","from nltk.corpus import stopwords\n","from tqdm.notebook import tqdm\n","import pyLDAvis.gensim_models as gensimvis\n","import pyLDAvis\n","from sklearn.feature_extraction.text import CountVectorizer\n","tqdm.pandas()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["NgrokTunnel: \"https://4d6c-172-83-13-4.ngrok-free.app\" -> \"http://localhost:8097\"\n"]},{"name":"stderr","output_type":"stream","text":["t=2023-09-06T23:13:28+0000 lvl=warn msg=\"failed to open private leg\" id=d1e7271a238b privaddr=localhost:8097 err=\"dial tcp 127.0.0.1:8097: connect: connection refused\"\n","t=2023-09-06T23:13:29+0000 lvl=warn msg=\"failed to open private leg\" id=fb3043e3f010 privaddr=localhost:8097 err=\"dial tcp 127.0.0.1:8097: connect: connection refused\"\n"]}],"source":["from pyngrok import conf, ngrok\n","import os\n","\n","conf.get_default().config_path = \"./ngrok/config.yml\"\n","\n","# Open a HTTP tunnel on the default port 80\n","public_url = ngrok.connect(8097)\n","print(public_url)"]},{"cell_type":"markdown","metadata":{"id":"ACSs5nIsh_TC"},"source":["# Path"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1693661922515,"user":{"displayName":"Hanschyo","userId":"06836591315332070494"},"user_tz":-120},"id":"oPeUIYRAiA1U","outputId":"b6c45ef1-b8bf-49e4-c53a-362e921bfec7"},"outputs":[],"source":["main_path = 'D:/Uni/Bachelorthesis/'"]},{"cell_type":"markdown","metadata":{"id":"lx6nnLksiZ4w"},"source":["# Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":88183,"status":"ok","timestamp":1693662112072,"user":{"displayName":"Hanschyo","userId":"06836591315332070494"},"user_tz":-120},"id":"gRoevEtribNC","outputId":"804becd7-62b3-4bbe-9266-9e2e1a6c8d6a"},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount('/content/drive')\n","df_kickstarter = pd.read_csv(main_path + 'kickstarter.csv')"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"pLhiCzo6mEBI"},"source":["# Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KXpAVfUTmEBJ"},"outputs":[],"source":["def numWords(text):\n","    r = Readability(text)\n","    try:\n","        t = r.statistics()\n","        return t['num_words']\n","    except:\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mzLqxygJmEBK"},"outputs":[],"source":["def removeHTML(html_str):\n","    soup = BeautifulSoup(str(html_str), features=\"html.parser\")\n","    return soup.get_text()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ks7n2rJfmEBL"},"outputs":[],"source":["def remove_urls(document):\n","    document = re.sub(r'http\\S+', '', str(document))\n","    return document"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"omBfO2_TmEBM"},"outputs":[],"source":["def remove_parting_lines(document):\n","    pattern = r'^[\\*\\-#]{5,}.+$'\n","    document = re.sub(pattern, '', str(document), flags=re.MULTILINE)\n","    return document"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r9EDnbHMmEBM"},"outputs":[],"source":["def detectLang(t):\n","    try:\n","        return detect(t)\n","    except:\n","        return None"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"BkUOGlUsmEBN"},"outputs":[],"source":["def preprocess_text(document: str, stemmer: nltk.stem.WordNetLemmatizer, en_stop: set) -> list:\n","    \"\"\"Preprocesses a document to remove special characters/whitespace/etc\n","\n","    Args:\n","        document (str):\n","        stemmer (nltk.stem.WordNetLemmatizer): Stemmer from NLTK\n","        en_stop (set): Set of stop words, usually from NLTK\n","\n","    Returns:\n","        str: preprocessed document\n","    \"\"\"\n","\n","    # Remove all the special characters\n","    document = re.sub(r'\\W', ' ', document)\n","\n","    # remove all single characters\n","    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n","\n","    # Remove single characters from the start\n","    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n","\n","    # Substituting multiple spaces with single space\n","    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n","\n","    # Removing prefixed 'b'\n","    document = re.sub(r'^b\\s+', '', document)\n","\n","    # Converting to Lowercase\n","    document = document.lower()\n","\n","    # Tokenization\n","    tokens = nltk.word_tokenize(document)\n","\n","    # POS-Tagging\n","    tagged_tokens = nltk.pos_tag(tokens)\n","\n","    # Filter Nouns and Lemmatization\n","    lemmatized_nouns = []\n","    for token, pos in tagged_tokens:\n","        if pos.startswith('N'):  # Check if the token is a noun\n","            lemma = stemmer.lemmatize(token)\n","            # FIlter stop words, words that contain only numbers and short words\n","            if lemma not in en_stop and not lemma.isdigit() and len(lemma) > 2:\n","                lemmatized_nouns.append(lemma)\n","\n","    return lemmatized_nouns;"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"-zvvGvaMmEBN"},"outputs":[],"source":["def preprocess_text_helper(t):\n","    stemmer = nltk.stem.WordNetLemmatizer()\n","    return preprocess_text(t, stemmer, stopwords.words('english'))"]},{"cell_type":"markdown","metadata":{"id":"77qd9KB1kFyp"},"source":["# General Cleaning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFkhN5BbmEBO"},"outputs":[],"source":["# Filter rows based on column: 'project_country'\n","df_kickstarter = df_kickstarter[df_kickstarter['project_country'] == \"US\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YR1VBja8mEBP"},"outputs":[],"source":["# Filter rows based on column: 'project_currency'\n","df_kickstarter = df_kickstarter[df_kickstarter['project_currency'] == \"USD\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sv61J78OmEBP"},"outputs":[],"source":["# Drop rows with missing data in column: 'project_title'\n","df_kickstarter = df_kickstarter.dropna(subset=['project_title'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XNwGErpOmEBP"},"outputs":[],"source":["# Drop rows with missing data in column: 'project_description'\n","df_kickstarter = df_kickstarter.dropna(subset=['project_title'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M4rv57k_mEBP"},"outputs":[],"source":["# Drop duplicate rows in column: 'project_description'\n","df_kickstarter = df_kickstarter.drop_duplicates(subset=['project_description'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tOa46R-omEBQ"},"outputs":[],"source":["df_kickstarter['project_description'] = df_kickstarter['project_description'].apply(removeHTML)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7hh5hTY_mEBQ"},"outputs":[],"source":["df_kickstarter['project_description'] = df_kickstarter['project_description'].apply(remove_urls)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0cuGAVXAmEBQ"},"outputs":[],"source":["df_kickstarter['project_description'] = df_kickstarter['project_description'].apply(remove_parting_lines)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QLzhmj69mEBQ"},"outputs":[],"source":["nltk.download('punkt')\n","df_kickstarter['num_words'] = df_kickstarter['project_description'].progress_apply(numWords)\n","df_kickstarter = df_kickstarter[df_kickstarter['num_words'] >= 100]\n","# Reducing outliers (very long project descriptions might effect topic modelling negatively)\n","df_kickstarter = df_kickstarter[df_kickstarter['num_words'] <= 3000]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0v3owT5mEBR"},"outputs":[],"source":["df_kickstarter['project_lang'] = df_kickstarter['project_description'].progress_apply(detectLang)\n","df_kickstarter = df_kickstarter[df_kickstarter['project_lang'] == 'en']"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"mUcaUhplmEBV"},"source":["# LDA"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["df_lda_kickstarter = pd.read_csv('./kickstarter_cleaned_topicmodelling.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1693661922516,"user":{"displayName":"Hanschyo","userId":"06836591315332070494"},"user_tz":-120},"id":"Xn4mtEzomEBW"},"outputs":[],"source":["# Copy Dataframe\n","# df_lda_kickstarter = df_kickstarter.copy()"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1693661922517,"user":{"displayName":"Hanschyo","userId":"06836591315332070494"},"user_tz":-120},"id":"W0h8S88gmEBb"},"outputs":[],"source":["# Remove the columns\n","df_lda_kickstarter.drop(df_lda_kickstarter.columns.difference(['project_description', 'project_category_id', 'project_parent_category_id', 'project_state']), axis=1,inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"xOZ9oyXumtzl"},"source":["### Technology Category"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1693661922517,"user":{"displayName":"Hanschyo","userId":"06836591315332070494"},"user_tz":-120},"id":"-gpcuFupl7RT"},"outputs":[],"source":["df_lda_kickstarter_technology = df_lda_kickstarter.copy()"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"iUu8VqLYyOUQ"},"outputs":[],"source":["# Filter Dataframe by Technology category (number 16)\n","df_lda_kickstarter_technology = df_lda_kickstarter_technology[(df_lda_kickstarter_technology['project_category_id'] == 16) | (df_lda_kickstarter_technology['project_parent_category_id'] == 16)]"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Z67x9LJ2-VWd"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\CoolerMaster\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\CoolerMaster\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\CoolerMaster\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f5da933a34c4f6585bd85b2ced82bcc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/26976 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Preprocessing general steps\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","df_lda_kickstarter_technology['project_description'] = df_lda_kickstarter_technology['project_description'].progress_apply(preprocess_text_helper)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Qf3CHYh2E8lB"},"outputs":[],"source":["data_technology = df_lda_kickstarter_technology['project_description'].tolist()\n","# Compute n-grams\n","from gensim.models import Phrases\n","\n","# Add n-grams to docs (only ones that appear 20 times or more).\n","ngrams_technology = Phrases(data_technology, min_count=20)\n","for idx in range(len(data_technology)):\n","    for token in ngrams_technology[data_technology[idx]]:\n","        if '_' in token:\n","            # Token is a n-grams, add to document.\n","            data_technology[idx].append(token)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"PUeg8iOxmEBc"},"outputs":[],"source":["# create dictionary\n","dictionary_technology = corpora.Dictionary(data_technology)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"dtciKE_2-AlX"},"outputs":[],"source":["# Filter out words that occur less than 5 documents, or more than 50% of the documents.\n","dictionary_technology.filter_extremes(no_below=5, no_above=0.5)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"iyq6wzLCCWlr"},"outputs":[],"source":["# Define your custom stoplist\n","custom_stoplist_technology = [\"project\", \"kickstarter\", \"pledge\", \"backer\", \"campaign\", \"product\"] #to be extended\n","# Add your custom stop words to the dictionary\n","stop_ids_technology = [dictionary_technology.token2id[word] for word in custom_stoplist_technology if word in dictionary_technology.token2id]\n","# Remove the stop words from the dictionary\n","dictionary_technology.filter_tokens(bad_ids=stop_ids_technology)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"sF-kLHTIBCfA"},"outputs":[],"source":["# create corpus\n","corpus_technology = [dictionary_technology.doc2bow(tokens) for tokens in data_technology]"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5563,"status":"ok","timestamp":1686749736312,"user":{"displayName":"Hanschyo","userId":"06836591315332070494"},"user_tz":-120},"id":"Esxs4nY6o3qb","outputId":"51f4045c-c31a-45bf-926a-10fda438b28d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('app', 39720), ('design', 31958), ('people', 31013), ('device', 30676), ('system', 29730), ('user', 28330), ('year', 27333), ('way', 26500), ('power', 24389), ('world', 24184), ('technology', 23965), ('video', 21718), ('goal', 20748), ('phone', 20672), ('play', 19988), ('part', 18539), ('life', 18421), ('feature', 18172), ('team', 18028), ('development', 17840), ('experience', 17774), ('day', 17587), ('software', 17564), ('data', 17200), ('business', 16974), ('battery', 16912), ('home', 16488), ('prototype', 15725), ('application', 15594), ('support', 15540), ('community', 15178), ('work', 15047), ('idea', 14839), ('cost', 14608), ('production', 14579), ('company', 14483), ('service', 14265), ('board', 14064), ('platform', 14049), ('level', 13721), ('thing', 13459), ('information', 13440), ('reward', 12976), ('tool', 12566), ('quality', 12455), ('game', 12197), ('water', 12163), ('order', 11971), ('friend', 11919), ('student', 11894), ('hand', 11880), ('control', 11869), ('computer', 11830), ('market', 11767), ('share', 11745), ('access', 11716), ('use', 11675), ('help', 11531), ('process', 11282), ('version', 11220), ('light', 11200), ('money', 11167), ('case', 11072), ('color', 11045), ('sensor', 10882), ('need', 10680), ('solution', 10535), ('camera', 10361), ('problem', 10358), ('place', 10087), ('medium', 10024), ('music', 9981), ('family', 9974), ('everyone', 9926), ('space', 9911), ('month', 9724), ('material', 9716), ('program', 9696), ('source', 9654), ('hour', 9610), ('model', 9557), ('event', 9554), ('child', 9264), ('option', 9170), ('website', 9021), ('something', 8863), ('browser', 8778), ('site', 8565), ('kit', 8563), ('school', 8524), ('car', 8517), ('fund', 8481), ('size', 8296), ('network', 8284), ('unit', 8200), ('number', 8160), ('customer', 8055), ('hardware', 8012), ('component', 7916), ('image', 7886)]\n"]}],"source":["from collections import Counter\n","\n","# Count word occurrences\n","word_counts_technology = Counter()\n","for doc in corpus_technology:\n","    for word_id, count in doc:\n","        word = dictionary_technology[word_id]\n","        word_counts_technology[word] += count\n","\n","# Get most frequent words\n","most_common_technology = word_counts_technology.most_common(100)\n","print(most_common_technology)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":69,"status":"ok","timestamp":1686749736313,"user":{"displayName":"Hanschyo","userId":"06836591315332070494"},"user_tz":-120},"id":"t42CpBmVBeLu","outputId":"326d0136-ee7d-44c4-a746-f134d508bd56"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of unique tokens: 19789\n","Number of documents: 26976\n"]}],"source":["print('Number of unique tokens: %d' % len(dictionary_technology))\n","print('Number of documents: %d' % len(corpus_technology))"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"wmF9pjY9J8Dh"},"outputs":[],"source":["# Initialize an empty list to store the LDA models\n","lda_models_technology = []"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["from gensim.models.callbacks import CoherenceMetric, DiffMetric, PerplexityMetric, ConvergenceMetric\n","\n","# define perplexity callback\n","pl = PerplexityMetric(corpus=corpus_technology, logger=\"visdom\", title=\"Perplexity\")\n","\n","# define other remaining metrics available\n","ch_umass = CoherenceMetric(corpus=corpus_technology, coherence=\"u_mass\", logger=\"visdom\", title=\"Coherence (u_mass)\")\n","ch_cv = CoherenceMetric(corpus=corpus_technology, texts=data_technology, coherence=\"c_v\", logger=\"visdom\", title=\"Coherence (c_v)\")\n","diff_kl = DiffMetric(distance=\"kullback_leibler\", logger=\"visdom\", title=\"Diff (kullback_leibler)\")\n","convergence_kl = ConvergenceMetric(distance=\"jaccard\", logger=\"visdom\", title=\"Convergence (jaccard)\")\n","\n","callbacks = [pl, ch_umass, ch_cv, diff_kl, convergence_kl]"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"vy75OBJFJ6BY"},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting up a new session...\n"]}],"source":["# Iterate over the desired topic counts from 10 to 300 in steps of 10\n","#for num_topics in tqdm(range(10, 301, 10)):\n","    # Create an LDA model with the current topic count\n","lda_model = gensim.models.LdaModel(corpus=corpus_technology, id2word=dictionary_technology, num_topics=100, passes=10, per_word_topics=True, chunksize=1500, iterations=150, alpha='auto', callbacks=callbacks)\n","\n","    # Save the model to Google Drive\n","    #model_path = main_path + f\"lda_technology/{num_topics}/lda_model_technology_{num_topics}\"\n","    #lda_model.save(model_path)\n","\n","    # Append the model path to the list\n","    #lda_models_technology.append(lda_model)"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\anaconda3\\envs\\Bachelorthesis\\lib\\site-packages\\pyLDAvis\\_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n","  default_term_info = default_term_info.sort_values(\n"]}],"source":["vis = gensimvis.prepare(lda_model, corpus_technology, dictionary_technology)\n","pyLDAvis.save_html(vis, './LDA/technology/vis/100_POS.html')"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["lda_model.save('./LDA/technology/100/LDA_technology_100_POS')"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["lda_technology_50 = gensim.models.LdaModel.load('./LDA/technology/50/LDA_technology_50')\n","lda_technology_100 = gensim.models.LdaModel.load('./LDA/technology/100/LDA_technology_100')\n","lda_technology_200 = gensim.models.LdaModel.load('./LDA/technology/200/LDA_technology_200')"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\anaconda3\\envs\\Bachelorthesis\\lib\\site-packages\\pyLDAvis\\_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n","  default_term_info = default_term_info.sort_values(\n","d:\\anaconda3\\envs\\Bachelorthesis\\lib\\site-packages\\pyLDAvis\\_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n","  default_term_info = default_term_info.sort_values(\n","d:\\anaconda3\\envs\\Bachelorthesis\\lib\\site-packages\\pyLDAvis\\_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n","  default_term_info = default_term_info.sort_values(\n"]}],"source":["vis_50 = gensimvis.prepare(lda_technology_50, corpus_technology, dictionary_technology)\n","vis_100 = gensimvis.prepare(lda_technology_100, corpus_technology, dictionary_technology)\n","vis_200 = gensimvis.prepare(lda_technology_200, corpus_technology, dictionary_technology)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["pyLDAvis.save_html(vis_50, './LDA/technology/vis/50.html')\n","pyLDAvis.save_html(vis_100, './LDA/technology/vis/100.html')\n","pyLDAvis.save_html(vis_200, './LDA/technology/vis/200.html')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HNZL8Yg4GAGe"},"outputs":[],"source":["# Loop through the created LDA models\n","for num_topics, lda_model in zip(range(10, 301, 10), lda_models_technology):\n","    # Create the visualization\n","    vis = gensimvis.prepare(lda_model, corpus_technology, dictionary_technology)\n","\n","    # Save the visualization as an HTML file\n","    html_path = main_path + f\"lda_technology/{num_topics}/lda_vis_technology_{num_topics}.html\"\n","    pyLDAvis.save_html(vis, html_path)"]},{"cell_type":"markdown","metadata":{"id":"3ih5dfcLdwhc"},"source":["### Games Category"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"rbu_Gjn4d2_C"},"outputs":[],"source":["df_lda_kickstarter_games = df_lda_kickstarter.copy()"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"Lnj8yfuAsbYm"},"outputs":[],"source":["# Filter Dataframe by Games category (number 12)\n","df_lda_kickstarter_games = df_lda_kickstarter_games[(df_lda_kickstarter_games['project_category_id'] == 12) | (df_lda_kickstarter_games['project_parent_category_id'] == 12)]"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"qmEJrLO9sds7"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b7d35b101d446a48f76a84f650ec323","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/39654 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Preprocessing\n","df_lda_kickstarter_games['project_description'] = df_lda_kickstarter_games['project_description'].progress_apply(preprocess_text_helper)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"V1WNGUAz1Mpq"},"outputs":[],"source":["data_games = df_lda_kickstarter_games['project_description'].tolist()\n","# Compute bigrams/trigrams\n","from gensim.models import Phrases\n","\n","# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n","ngrams_games = Phrases(data_games, min_count=20)\n","for idx in range(len(data_games)):\n","    for token in ngrams_games[data_games[idx]]:\n","        if '_' in token:\n","            # Token is a bigram/trigram, add to document.\n","            data_games[idx].append(token)"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"pkQa293qX4OC"},"outputs":[],"source":["# create dictionary\n","dictionary_games = corpora.Dictionary(data_games)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"010Cld_k1ddq"},"outputs":[],"source":["# Filter out words that occur less than 5 documents, or more than 50% of the documents.\n","dictionary_games.filter_extremes(no_below=5, no_above=0.5)"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"XCGoyN7I1d7C"},"outputs":[],"source":["# Define your custom stoplist\n","custom_stoplist_games = [\"project\", \"kickstarter\", \"pledge\", \"backer\", \"campaign\", \"product\"] #to be extended\n","# Add your custom stop words to the dictionary\n","stop_ids_games = [dictionary_games.token2id[word] for word in custom_stoplist_games if word in dictionary_games.token2id]\n","# Remove the stop words from the dictionary\n","dictionary_games.filter_tokens(bad_ids=stop_ids_games)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"rbbQ1kfd1g9H"},"outputs":[],"source":["# create corpus\n","corpus_games = [dictionary_games.doc2bow(tokens) for tokens in data_games]"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"SwxR2mZ6xuq7"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('card', 187046), ('character', 65596), ('level', 62734), ('deck', 61114), ('book', 48977), ('reward', 48967), ('play', 44231), ('art', 41949), ('design', 41142), ('year', 40574), ('stretch', 39548), ('dice', 38203), ('stretch_goal', 37227), ('people', 35017), ('story', 34738), ('copy', 34333), ('board', 34033), ('rule', 33813), ('adventure', 32497), ('shipping', 32293), ('system', 32043), ('team', 31380), ('box', 30285), ('set', 30063), ('edition', 29871), ('version', 29191), ('item', 29123), ('cost', 28641), ('life', 27998), ('point', 27868), ('friend', 27775), ('order', 27523), ('thing', 27511), ('color', 26079), ('experience', 25413), ('hand', 25394), ('part', 25250), ('work', 25206), ('page', 25084), ('day', 23422), ('power', 23205), ('action', 22846), ('monster', 22786), ('video', 22296), ('print', 22199), ('ability', 21917), ('number', 21794), ('battle', 21661), ('map', 21426), ('idea', 20619), ('support', 20463), ('piece', 20376), ('everyone', 20289), ('fun', 20132), ('hero', 20056), ('artist', 19675), ('city', 19369), ('development', 19351), ('option', 19307), ('name', 19167), ('something', 18356), ('expansion', 18323), ('place', 18318), ('quality', 18025), ('tier', 17850), ('end', 17599), ('space', 17509), ('money', 17461), ('everything', 17113), ('lot', 17056), ('pdf', 17003), ('pack', 16949), ('community', 16809), ('turn', 16420), ('war', 16169), ('dungeon', 16168), ('family', 16051), ('custom', 16021), ('event', 15952), ('enemy', 15775), ('amount', 15760), ('miniature', 15755), ('style', 15456), ('group', 15451), ('weapon', 15445), ('designer', 15439), ('production', 15231), ('company', 15206), ('funding', 15052), ('help', 14977), ('skill', 14900), ('content', 14668), ('base', 14483), ('feature', 14426), ('combat', 14291), ('type', 14149), ('side', 14100), ('choice', 13929), ('party', 13889), ('add', 13802)]\n"]}],"source":["from collections import Counter\n","\n","# Count word occurrences\n","word_counts_games = Counter()\n","for doc in corpus_games:\n","    for word_id, count in doc:\n","        word = dictionary_games[word_id]\n","        word_counts_games[word] += count\n","\n","# Get most frequent words\n","most_common_games = word_counts_games.most_common(100)\n","print(most_common_games)"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49,"status":"ok","timestamp":1686750154115,"user":{"displayName":"Hanschyo","userId":"06836591315332070494"},"user_tz":-120},"id":"ReG56hlI8sbj","outputId":"ed057706-cddd-471c-c0cf-12ee4e9287fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of unique tokens: 31660\n","Number of documents: 39654\n"]}],"source":["print('Number of unique tokens: %d' % len(dictionary_games))\n","print('Number of documents: %d' % len(corpus_games))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oC7_PSQ2YGUD"},"outputs":[],"source":["# Initialize an empty list to store the LDA models\n","lda_models_games = []"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["from gensim.models.callbacks import CoherenceMetric, DiffMetric, PerplexityMetric, ConvergenceMetric\n","\n","# define perplexity callback\n","pl = PerplexityMetric(corpus=corpus_games, logger=\"visdom\", title=\"Perplexity\")\n","\n","# define other remaining metrics available\n","ch_umass = CoherenceMetric(corpus=corpus_games, coherence=\"u_mass\", logger=\"visdom\", title=\"Coherence (u_mass)\")\n","ch_cv = CoherenceMetric(corpus=corpus_games, texts=data_games, coherence=\"c_v\", logger=\"visdom\", title=\"Coherence (c_v)\")\n","diff_kl = DiffMetric(distance=\"kullback_leibler\", logger=\"visdom\", title=\"Diff (kullback_leibler)\")\n","convergence_kl = ConvergenceMetric(distance=\"jaccard\", logger=\"visdom\", title=\"Convergence (jaccard)\")\n","\n","callbacks = [pl, ch_umass, ch_cv, diff_kl, convergence_kl]"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["lda_model_70 = gensim.models.LdaModel(corpus=corpus_games, id2word=dictionary_games, num_topics=70, passes=5, per_word_topics=True, chunksize=2000, iterations=150, alpha='auto')"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["lda_model_100 = gensim.models.LdaModel(corpus=corpus_games, id2word=dictionary_games, num_topics=100, passes=3, per_word_topics=True, chunksize=20000, iterations=150, alpha='auto')"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["lda_model_125 = gensim.models.LdaModel(corpus=corpus_games, id2word=dictionary_games, num_topics=125, passes=3, per_word_topics=True, chunksize=20000, iterations=150, alpha='auto')"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\anaconda3\\envs\\Bachelorthesis\\lib\\site-packages\\pyLDAvis\\_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n","  default_term_info = default_term_info.sort_values(\n"]}],"source":["vis = gensimvis.prepare(lda_model_70, corpus_games, dictionary_games)\n","pyLDAvis.save_html(vis, './LDA/games/vis/70_POS.html')"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\anaconda3\\envs\\Bachelorthesis\\lib\\site-packages\\pyLDAvis\\_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n","  default_term_info = default_term_info.sort_values(\n","d:\\anaconda3\\envs\\Bachelorthesis\\lib\\site-packages\\pyLDAvis\\_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n","  default_term_info = default_term_info.sort_values(\n","d:\\anaconda3\\envs\\Bachelorthesis\\lib\\site-packages\\pyLDAvis\\_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n","  default_term_info = default_term_info.sort_values(\n"]}],"source":["vis_75 = gensimvis.prepare(lda_model_75, corpus_games, dictionary_games)\n","vis_100 = gensimvis.prepare(lda_model_100, corpus_games, dictionary_games)\n","vis_125 = gensimvis.prepare(lda_model_125, corpus_games, dictionary_games)"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["pyLDAvis.save_html(vis_75, './LDA/games/vis/75.html')\n","pyLDAvis.save_html(vis_100, './LDA/games/vis/100.html')\n","pyLDAvis.save_html(vis_125, './LDA/games/vis/125.html')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gzOMy2KtYOLK"},"outputs":[],"source":["# Iterate over the desired topic counts from 10 to 300 in steps of 10\n","for num_topics in range(10, 301, 10):\n","    # Create an LDA model with the current topic count\n","    lda_model = gensim.models.LdaMulticore(corpus=corpus_games, id2word=dictionary_games, num_topics=num_topics, passes=30, workers=8, per_word_topics=True, chunksize=100, iterations=150, eval_every=None, gamma_threshold=0.001)\n","\n","    # Save the model to Google Drive\n","    model_path = main_path + f\"lda_games/{num_topics}/lda_model_games_{num_topics}\"\n","    lda_model.save(model_path)\n","\n","    # Append the model path to the list\n","    lda_models_games.append(lda_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ic3X9u3YhA3"},"outputs":[],"source":["# Loop through the created LDA models\n","for num_topics, lda_model in zip(range(10, 301, 10), lda_models_games):\n","    # Create the visualization\n","    vis = gensimvis.prepare(lda_model, corpus_games, dictionary_games)\n","\n","    # Save the visualization as an HTML file\n","    html_path = main_path + f\"lda_games/{num_topics}/lda_vis_games_{num_topics}.html\"\n","    pyLDAvis.save_html(vis, html_path)"]},{"cell_type":"markdown","metadata":{"id":"z2kFHx1e5QDS"},"source":["### Design Category"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"7gAw4I_P7HwW"},"outputs":[],"source":["df_lda_kickstarter_design = df_lda_kickstarter.copy()"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"ogi251Qd7Hwk"},"outputs":[],"source":["# Filter Dataframe by Design category (number 7)\n","df_lda_kickstarter_design = df_lda_kickstarter_design[(df_lda_kickstarter_design['project_category_id'] == 7) | (df_lda_kickstarter_design['project_parent_category_id'] == 7)]"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"X5m_Oul17Hwl"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\CoolerMaster\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\CoolerMaster\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18f7a3ac73ab43f7aec8efc451c34d2e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/28290 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Preprocessing general steps\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","df_lda_kickstarter_design['project_description'] = df_lda_kickstarter_design['project_description'].progress_apply(preprocess_text_helper)"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"EqFabi7p7Hwm"},"outputs":[],"source":["data_design = df_lda_kickstarter_design['project_description'].tolist()\n","# Compute n-grams\n","from gensim.models import Phrases\n","\n","# Add n-grams to docs (only ones that appear 20 times or more).\n","ngrams_design = Phrases(data_design, min_count=20)\n","for idx in range(len(data_design)):\n","    for token in ngrams_design[data_design[idx]]:\n","        if '_' in token:\n","            # Token is a n-grams, add to document.\n","            data_design[idx].append(token)"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"WvWVbiXr7Hwn"},"outputs":[],"source":["# create dictionary\n","dictionary_design = corpora.Dictionary(data_design)"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"LIa4iNif7Hwn"},"outputs":[],"source":["# Filter out words that occur less than 5 documents, or more than 50% of the documents.\n","dictionary_design.filter_extremes(no_below=5, no_above=0.5)"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"wri17dHo7Hwo"},"outputs":[],"source":["# Define your custom stoplist\n","custom_stoplist_design = [\"project\", \"kickstarter\", \"pledge\", \"backer\", \"campaign\", \"product\"] #to be extended\n","# Add your custom stop words to the dictionary\n","stop_ids_design = [dictionary_design.token2id[word] for word in custom_stoplist_design if word in dictionary_design.token2id]\n","# Remove the stop words from the dictionary\n","dictionary_design.filter_tokens(bad_ids=stop_ids_design)"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"qPAAP6_I7Hwo"},"outputs":[],"source":["# create corpus\n","corpus_design = [dictionary_design.doc2bow(tokens) for tokens in data_design]"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6765,"status":"ok","timestamp":1686750696337,"user":{"displayName":"Hanschyo","userId":"06836591315332070494"},"user_tz":-120},"id":"OAnSwH0M7Hwo","outputId":"26a7349b-2660-445e-a802-91157a7dd82b"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('year', 29868), ('way', 26199), ('production', 25489), ('color', 25434), ('material', 23784), ('goal', 23654), ('hand', 22765), ('people', 21379), ('day', 21053), ('reward', 20921), ('quality', 20735), ('bag', 20164), ('prototype', 19939), ('world', 19687), ('life', 19384), ('case', 17866), ('order', 17849), ('water', 17793), ('part', 17763), ('play', 17668), ('size', 17603), ('idea', 16367), ('card', 16111), ('support', 15584), ('work', 15074), ('process', 14131), ('home', 14068), ('tool', 13981), ('system', 13738), ('thing', 13736), ('cost', 13113), ('company', 12783), ('pocket', 12689), ('use', 12287), ('phone', 12187), ('experience', 11951), ('option', 11928), ('friend', 11918), ('piece', 11869), ('place', 11760), ('space', 11305), ('video', 11272), ('something', 11270), ('help', 11210), ('steel', 11046), ('business', 10898), ('market', 10878), ('team', 10829), ('level', 10716), ('device', 10470), ('wallet', 10402), ('family', 10353), ('watch', 10252), ('line', 10079), ('shipping', 9994), ('side', 9992), ('box', 9850), ('feature', 9727), ('model', 9563), ('bottle', 9448), ('body', 9440), ('child', 9410), ('end', 9401), ('item', 9174), ('power', 9156), ('community', 8999), ('need', 8918), ('style', 8860), ('problem', 8848), ('everyone', 8835), ('art', 8759), ('month', 8703), ('price', 8681), ('share', 8588), ('custom', 8541), ('strap', 8428), ('manufacturing', 8425), ('lot', 8406), ('money', 8179), ('wood', 7991), ('board', 7987), ('light', 7976), ('plastic', 7892), ('solution', 7857), ('fund', 7809), ('kid', 7749), ('version', 7656), ('thank', 7536), ('pack', 7508), ('technology', 7455), ('everything', 7412), ('inch', 7410), ('car', 7392), ('page', 7316), ('book', 7275), ('surface', 7241), ('development', 7192), ('bike', 7134), ('funding', 7126), ('hour', 7117)]\n"]}],"source":["from collections import Counter\n","\n","# Count word occurrences\n","word_counts_design = Counter()\n","for doc in corpus_design:\n","    for word_id, count in doc:\n","        word = dictionary_design[word_id]\n","        word_counts_design[word] += count\n","\n","# Get most frequent words\n","most_common_design = word_counts_design.most_common(100)\n","print(most_common_design)"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48,"status":"ok","timestamp":1686750696338,"user":{"displayName":"Hanschyo","userId":"06836591315332070494"},"user_tz":-120},"id":"nkeAqDhp7Hwp","outputId":"fe32503b-9642-40d7-da20-22d8fa977846"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of unique tokens: 21627\n","Number of documents: 28290\n"]}],"source":["print('Number of unique tokens: %d' % len(dictionary_design))\n","print('Number of documents: %d' % len(corpus_design))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xbySN96M7Hwp"},"outputs":[],"source":["# Initialize an empty list to store the LDA models\n","lda_models_design = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from gensim.models.callbacks import CoherenceMetric, DiffMetric, PerplexityMetric, ConvergenceMetric\n","\n","# define perplexity callback\n","pl = PerplexityMetric(corpus=corpus_design, logger=\"visdom\", title=\"Perplexity\")\n","\n","# define other remaining metrics available\n","ch_umass = CoherenceMetric(corpus=corpus_design, coherence=\"u_mass\", logger=\"visdom\", title=\"Coherence (u_mass)\")\n","ch_cv = CoherenceMetric(corpus=corpus_design, texts=data_design, coherence=\"c_v\", logger=\"visdom\", title=\"Coherence (c_v)\")\n","diff_kl = DiffMetric(distance=\"kullback_leibler\", logger=\"visdom\", title=\"Diff (kullback_leibler)\")\n","convergence_kl = ConvergenceMetric(distance=\"jaccard\", logger=\"visdom\", title=\"Convergence (jaccard)\")\n","\n","callbacks = [pl, ch_umass, ch_cv, diff_kl, convergence_kl]"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":["lda_model_75 = gensim.models.LdaModel(corpus=corpus_design, id2word=dictionary_design, num_topics=75, passes=3, per_word_topics=True, chunksize=15000, iterations=150, alpha='auto')"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["lda_model_100 = gensim.models.LdaModel(corpus=corpus_design, id2word=dictionary_design, num_topics=100, passes=5, per_word_topics=True, chunksize=1500, iterations=150, alpha='auto')"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":["lda_model_125 = gensim.models.LdaModel(corpus=corpus_design, id2word=dictionary_design, num_topics=125, passes=3, per_word_topics=True, chunksize=15000, iterations=150, alpha='auto')"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\anaconda3\\envs\\Bachelorthesis\\lib\\site-packages\\pyLDAvis\\_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n","  default_term_info = default_term_info.sort_values(\n"]}],"source":["vis = gensimvis.prepare(lda_model_100, corpus_design, dictionary_design)\n","pyLDAvis.save_html(vis, './LDA/design/vis/100_POS.html')"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\anaconda3\\envs\\Bachelorthesis\\lib\\site-packages\\pyLDAvis\\_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n","  default_term_info = default_term_info.sort_values(\n","d:\\anaconda3\\envs\\Bachelorthesis\\lib\\site-packages\\pyLDAvis\\_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n","  default_term_info = default_term_info.sort_values(\n","d:\\anaconda3\\envs\\Bachelorthesis\\lib\\site-packages\\pyLDAvis\\_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n","  default_term_info = default_term_info.sort_values(\n"]}],"source":["vis_75 = gensimvis.prepare(lda_model_75, corpus_design, dictionary_design)\n","vis_100 = gensimvis.prepare(lda_model_100, corpus_design, dictionary_design)\n","vis_125 = gensimvis.prepare(lda_model_125, corpus_design, dictionary_design)"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[],"source":["pyLDAvis.save_html(vis_75, './LDA/design/vis/75.html')\n","pyLDAvis.save_html(vis_100, './LDA/design/vis/100.html')\n","pyLDAvis.save_html(vis_125, './LDA/design/vis/125.html')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mxzExUEo7Hwp"},"outputs":[],"source":["# Iterate over the desired topic counts from 10 to 300 in steps of 10\n","for num_topics in tqdm(range(10, 301, 10)):\n","    # Create an LDA model with the current topic count\n","    lda_model = gensim.models.LdaMulticore(corpus=corpus_design, id2word=dictionary_design, num_topics=num_topics, passes=30, workers=8, per_word_topics=True, chunksize=100, iterations=150, eval_every=None, gamma_threshold=0.001)\n","\n","    # Save the model to Google Drive\n","    model_path = main_path + f\"lda_design/{num_topics}/lda_model_design_{num_topics}\"\n","    lda_model.save(model_path)\n","\n","    # Append the model path to the list\n","    lda_models_design.append(lda_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBXFYi4x7Hwq"},"outputs":[],"source":["# Loop through the created LDA models\n","for num_topics, lda_model in zip(range(10, 301, 10), lda_models_design):\n","    # Create the visualization\n","    vis = gensimvis.prepare(lda_model, corpus_design, dictionary_design)\n","\n","    # Save the visualization as an HTML file\n","    html_path = main_path + f\"lda_design/{num_topics}/lda_vis_design_{num_topics}.html\"\n","    pyLDAvis.save_html(vis, html_path)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPm1uRw+MpIU9iuzSTGl6Zy","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":0}
